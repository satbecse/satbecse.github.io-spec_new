<html>
<style>
  html,
  body {
    height: 100%;
  }

  .button {
    background-color: #008CBA;
    /* Blue */
    border: none;
    color: white;
    padding: 15px 32px;
    text-align: center;
    text-decoration: none;
    display: inline-block;
    font-size: 16px;
    margin: 4px 2px;
    cursor: pointer;
  }

  .button1 {
    background-color: #4CAF50;
    /*green*/
  }

  /*.button2 {
    background-color: #f44336;
     Red 
  }*/
</style>

<head>
  <title> Spectrogram Generator</title>
</head>

<body>
  <button class="button button1" id='startbutton' onclick=initialize()>Start Listening</button>
  <!--<button class="button button2" id='stopbutton' onclick=stop()>Stop Listening</button>-->
  <canvas></canvas>
</body>
<script>
  /*  Parameters to be passed
  // Number of Frames    56    (minimum=10 )
  // Overlap Factor      0.25  (0 to 1)
  // FFT Size            (default : 2048 power of 2)      
  // FrameDuration       (23.21)
  */
  console.clear();
  let audioContext=null, analyser, data, length, source, frameIntervalTask, streaming = false, sampling_end = false;
  let freqQueue = []
  
  function initialize() {
    if (isListening()) {
      stop()
      document.getElementById('startbutton').textContent = 'Start Listening'
      document.getElementById('startbutton').style.backgroundColor = '#4CAF50';
      return
    }
    //document.getElementById('startbutton').disabled = true;          //Sathish:Disabling the start button
    document.getElementById('startbutton').textContent = 'Stop Listening'
    document.getElementById('startbutton').style.backgroundColor = '#f44336';
    if (document.getElementById('startbutton')){
      audioContext = new (window.AudioContext || window.webkitAudioContext)   //Sathish : Creating an AudioContext 
      console.log('I am here')
    }
    analyser = audioContext.createAnalyser();                          //Sathish : Creating an Analysyer 
    //analyser.fftSize = 4096;  
    analyser.fftSize = 2048;                                         //Sathish : Setting the FFT Size 
    navigator.mediaDevices.getUserMedia({ audio: true })             // Sathish : Accessing the Mic           
      .then(process) //.catch((errorMessage)=>{console.log('Error Occured:',errorMessage)});                                                  
  }

  function process(stream) {
    if (streaming) {
      throw new Error('Cannot start streaming again when streaming is ongoing.');
    }
    streaming = true
    source = audioContext.createMediaStreamSource(stream);
    source.connect(analyser);
    // const data = new Uint8Array(analyser.frequencyBinCount);   // Sathish : To get the FFT data in Uint8Array
    data = new Float32Array(analyser.frequencyBinCount);         // Sathish :To get FFT data in float32array from audio context analyser node
    length = data.length;
    console.log('length:', length);
    console.log((analyser.fftSize / 2) / audioContext.sampleRate * 1e3)
    frameIntervalTask = setInterval(getFrequencyData, (analyser.fftSize / 2) / audioContext.sampleRate * 1e3)    // Doubt : This gives me 23.21 
    //let timeout = setTimeout(stop, 2600)
  }

  function isListening() {
    return streaming;
  }

  let counter = 0;
  function getFrequencyData() {
    // analyser.getByteFrequencyData(data); 
    analyser.getFloatFrequencyData(data)      // To get the float data  //calculates FFT and stores in data  --> what is the Size of the data ? 
    //  console.log('data', data)             // Printing the data
   // console.log ('maximum',Math.max(data))
   // console.log('minimum',Math.min(data))
    if (data[0] == -Infinity || 0)
      return;
    freqQueue.push(data.slice(0,232))
    if (freqQueue.length > 55) {
      flat_queue = flattenQueue(freqQueue)
      normalizedData = normalize(flat_queue)
      console.log('normalizedData or spectrogramData :', counter++, normalizedData)
      //console.log('freqQueue:', freqQueue)                                    
      const deleteCount = Math.floor(freqQueue.length * (1 - 0.75));    // 0.25 is a overlapfactor   
      freqQueue.splice(0, deleteCount)
    }
  }

  function normalize(x) {
    const mean = -100;
    const std = 10;
    return x.map(x => (x - mean) / std);
  }

  function flattenQueue(queue) {
    var frameSize = queue[0].length;        //queue[0].length=232
    var freqData = new Float32Array(queue.length * frameSize);  //56*232queue.length=56 queue[0].length=232
    queue.forEach(function (data, i) { return freqData.set(data, i * frameSize); });
    return freqData;
  }

  function stop() {
    if (frameIntervalTask == null) {
      throw new Error('Cannot stop because there is no ongoing streaming activity.');
    }
    clearInterval(frameIntervalTask)
    analyser.disconnect();
    audioContext.close();
    if (source != null && source.mediaStream.getTracks().length > 0) {
      source.mediaStream.getTracks()[0].stop();
      console.log("Stopping the stream")
    }
    console.log('I am stopped')
    streaming = false
    counter = 0;
  }
</script>
</html>